# Kaggle Natural Language Processing Notes
  
__NB: Parts of these notes are DIRECTLY COPIED from the [Kaggle Natural Language Processing Course](https://www.kaggle.com/learn/natural-language-processing), for example definitions etc.__
  
__WARNING: This course uses spaCy 2.3.5 - spaCy v3.0 alters the syntax hugely so make sure to be using spaCy 2.3.5__

## Intro to Natural Language Processing (NLP)
  
The leading Python library for __Natural Language Processing (NLP)__ is `spaCy`. It can be used for:
  
* Basic processing of text and pattern matching.
* Building machine learning models with text.
* Representing text with word embeddings that numerically capture the meaning of words and documents.
  
[Link to spaCy documentation](https://spacy.io/usage)
  
spaCy is dependent on models that are language-specific and come in a variety of different sizes. Models are loaded using `spacy.load`. For example to load the English language model: `import spacy \n nlp = spacy.load('en')`. Once the model is loaded text is processed as follows: `doc = nlp("This is some text, is it interesting?")`.
  
  
### Tokenizing
  
The `doc` object is a _document object_ that contains __tokens__. Tokens are single units of text in the document, such as individual words and punctuation. NB: spaCy splits contractions like "don't" into 2 separate tokens: "do" and "n't". The tokens can be viewed by iterating through the document object, what is displayed are token objects. Each token has additional information for example: `token.lemma_` and `token.is_stop`. (These are explained below).
  
  
### Text Preprocessing
  
There are several types of preprocessing that help to improve how we model with words.
  
* __"Lemmatising"__ is the process of converting a word into its "base" form. For example, "walking" lemmatised is "walk". The word in its base form is known as the "lemma" of a word. `token.lemma_` returns the lemma of the token.
* __"Stopwords"__ are frequently occuring words that do not contain much information. (It is common to remove these). Examples include: "the", "is", "and", "but", "not". `token.is_stop` returns a boolean `True` or `False` if the token is a stopword or not.
  
Language data has a large amount of _noise_ alongside the informative content. Lemmatising can help by combining different forms of the same word into one base form. (e.g. "calming", "calms", and "calmed" would all change to "calm"). At the same time, removing stop words can help the model to focus on the most relevant words.
  
NB: Lemmatising and removing stopwords can sometimes result in worse performance for a model. As such, this preprocessing should simply be part of the hyperparameter optimisation process.
  
  
### Pattern Matching
  
Matching tokens or phrases within text/documents is another common NLP task. This can be done using regular expressions however, spaCy's functionality for matching tends to be easier to use.
  
A `Matcher` is used to match individual tokens. When trying to match a list of terms, it is both easier and more efficient to use `PhrasMatcher`. The matcher is generated using the vocabulary of the model (For example the English Language model). The `attr='LOWER'` means that phrases are matched on lowercased text. (i.e. the matching is case insensitive). The phrase matcher requires patterns as document objects - these can be generated using the original `nlp` model with a list comprehension. 
  
  
  
## Text Classification
  
One of the most common tasks in Natural Language Processing is __text classificaiton__. This is like normal Machine Learning "classification" and is applied to text. For example, spam detection, sentiment analysis, and tagging customer queries. `spaCy` can be used for text classification. This module of the course looks at detecting spam messages, similar to the type of spam detection engines in email clients.
  
  
### Bag of Words
  
ML models can't learn from raw text data; the text must be converted into something numeric.
  
One of the most common representations of text data is a form of one-hot encoding. Documents are represented as a vector of term frequencies for each term in the vocabulary. (The vocabulary is built from the tokens in the "corpus" - the corpus is the collection of documents). This representation is as follows: for each document...
  
* Count the number of times a term occurs.
* Store the count in the appropriate element of the a vector.
  
  
Corpus of: "Tea is life. Tea is love." (Which is true) and "Tea is healthy, calming, and delicious."  
Vocabulary: {"tea", "is", "life", "love", "healthy", "calming", "and", "delicious"} (Ignoring punctuation).  
Vectors: `v<sub>1</sub> = [2, 2, 1, 1, 0, 0, 0, 0], v<sub>2</sub> = [1, 1, 0, 0, 1, 1, 1, 1]`
  
  
This form of representation is called the __bag of words__ representation. Documents that share similar terms will have similar vectors. NB: vocabularies usually contain thousands of terms, as such the vectors can be very large.
  
  
An alternative common representation is __TF-IDF (Term Frequency - Inverse Document Frequency)__. This is similar to a bag of words however each term count is scaled by the term's frequency in the overall corpus. TF-IDF can sometimes help to improve models.
  
### Building a Bag of Words model
  
The first step is to represent the documents in a bag of words representation, the vectors can then be used as an input for any machine learning model. The spaCy `TextCategorizer` class deals with both the bag of words conversion and the building of a simple linear model. This class is a spaCy __pipe__ - pipes are specific classes that are used for processing and transforming tokens.
  
There are several default pipes with the basic spaCy model (`nlp = spacy.load('en_core_web_sm')`). These pipes perform things like speech tagging, entity recognition, and other different transformations. When text is run through a model (`doc = nlp('Hello, this is text!')`), the outputs of these pipes are linked to the tokens in the `doc` object. (e.g. the lemmas for `token.lemma_` come from a pipe). Pipes can be removed and added to models.
  
* In the case of the "spam" dataset the classes are either "ham" or "spam" - as such `"exclusive_classes"` is set to `True`.
* The architecutre is set to `"bow"` ("Bag of words").
  
spaCy can provide a convolutional neural network architecture.
  

### Training a "Text Categorizer" Model
  
The labels in the data must be converted into the form that the `TextCategorizer` needs. This is done by creating a dictionary of boolean values for each class, for each document. (For example, if the category is "ham" then the dictionary would need to be: `{"ham": True, "ham": False}`). These are stored inside a dictionary with the key `"cats"` (categories). The text and the labels are combined into a single list.
  
Once the data has been converted into the correct form the model can be trained:
  
An `optimizer` is created using `nlp.begin_training()`. This optimizer is used by spaCy to update the model. It is usually more efficient to train models in small "batches". spaCy uses the `minibatch` function which essentially generates minibatches for training. The final step is to split the minibatches into texts and labels. `nlp.update` updates the model's parameters.
  
The 6th cell in the tutorial code is for a single epoch (training loop) throught the data. Another loop is used for more epochs. (It can also be useful to re-shuffle the training data at the begining of each loop).
  
### Making Predictions
  
Predictions are made using the `predict()` method. Any input text must be tokenised with `nlp.tokenizer`. The tokens are then passed to the predict method which returns scores. (Scores are the probability the input text belonds to the classes in the model).
  
Use the scores to predict a single class/label by choosing the label with the highest probability. The index of the highest probability is found using `scores.argmax`, this index can be used to get the label string from `textcat.labels`.
  
To find the accuracy of the model, calculate the number of correct predictions made on the test data and divide it by the total number of predictions.
  
  
Models can be optimised further by adjusting different hyperparameters. The most important of these is the `TextCategorizer` architecture: the CNN and ensemble models in general have better performance than the BOW models but take longer to train.
  
## Word Vectors
  

### Word Embeddings
  
__Word embeddings__ (aka word vectors) represent each word numerically. This is done so that the vectors correspond to either how the word is used or what the word means. The vector encodings are learned by assessing the context in which the different words appear. As such, words that appear in similar contexts will in turn have similar vectors. (For example, vectors for "carrot", "potato", "beans" will be pretty close together but they will be far away from vectors like "moon" and "city").
  
The relations between words can be examined using mathematical operations. For example, if you subtract the vectors for "man" and "woman" this will return another vector. Add this vector to the vector for "king" and the result will be fairly close to the vector for "queen".
  
  
![Examples of Linear Relationships](/files/linear-relationships.png)
  
_The image above is taken from the [Kaggle Natural Language Processing Course](https://www.kaggle.com/learn/natural-language-processing) all rights reserved to Kaggle, Dan Becker (Data Scientist - Instructor for this course) and Mat Leonard (AI Educator - Instructor for this course)._
  
These types of vectors can be used as features for machine learning models. Word vectors are particularly useful as they will usually improve model performance beyond when using "Bag of Words" encoding. spaCy provides embeddings learned from a model called __Word2Vec__ and these can be accessed by loading a "large" language model. For example `en_core_web_lg` then use the `.vector` attribute on the tokens.
  
__WARNING: The `en_core_web_lg` language model takes up 782.7 MB!__
  
The vectors are 300-dimensional, with a vector per word. The issue is that the labels are document-level and as such the models can't use any word-level embeddings. Therefore, a vector representatoin of the entire document is needed.
  
There are lots of ways of combining all of the word vectors into a single document vector. The single document vector can then be used for model training. The simplest approach is actually just to average the vectors for each word in the document. These document vectors can then be used for modeling. spaCy can calculate the average document vector with `doc.vector`.
  

#### Classification Models
  

We can use the document vectors to train `scikit-learn` models, `xgboost` models, or basically any other normal approach to modeling. The tutorial code uses the Scikit-learn SVM (Support Vector Machine) classifier `LinearSVC`, this works in a very similar way other scikit-learn models.

#### Document Similarity
  
Documents that contain similar content usually have similar vectors. It is possible to find similar documents by measuring the similarity between the vectors. One of the most common metrics for this is __cosine similarity__. This measure the angle between two vectors __a__ and __b__. The calculation is the dot product of __a__ and __b__ divide by the product of the magnitudes of each vector. The cosinge similarity varies between -1 (complete opposite) and 1 (perfect similarity). It can be calculated either using a self defined function or the scikit-learn metric (`sklearn.metrics.pairwise.cosine_similarity(X, Y=None, dense_output=True)`).
  
  
![Certificate for completing the "Natural Language Processing" course.](/files/EdwardSleath_NLP.png)
  

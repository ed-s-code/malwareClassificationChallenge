# Kaggle Intro to Machine Learning Notes
  
__NB: Parts of these notes are DIRECTLY COPIED from the [Kaggle Intro to Machine Learning Course](https://www.kaggle.com/learn/intro-to-machine-learning), for example definitions etc.__

## How Models Work

1. Define a model   -  `my_model = ModelName()`
2. Fit a model      -  `my_model.fit(features,target)`
3. Make predictions -  `my_model.predict(data)`
4. Validate our model
  

Decision Trees are a type of model.
  
Capturing patterns from data is called fitting or training the model.
* The data used to fit the model is the "training" data.
  
Once the model has been fit, you can apply it to new data to make predictions. 
  
The point at the bottom, where we make a prediciton, is called a leaf.
  
## Basic Data Exploration
  
* `pandas` is the libraryused for data analysis and manipulation. (Abbreviated in code as `pd`).
* DataFrames hold the data in the form of a table. The data is loaded into a DataFrame used the `pd.read_csv()` function.
* See the [BDEtutorial notebook](BDEtutorial.ipynb) for the tutorial code.
  
`data.shape` returns the size of the dataframe as: `(x rows, y columns)`.  
`data.columns` returns the names of the variables in the dataframe.  
`data.head` returns the first few rows of the dataframe.  
`data.isnull().sum()` checks for missing values in the dataframe.  
`data.describe()` prints a summary table of the dataframe.  

## Your First Machine Learning Model
  
use `data.columns` to help choose suitable variables for the model.
  
`melbourne_data = melbourne_data.dropna(axis=0)` drops houses which include missing values from the dataframe.
  
Approaches to select a subset of data:
1. Dot notation, which we use to select the "prediction target"
2. Selecting with a column list, which we use to select the "features"
  
* See the [YFMLMtutorial notebook](YFMLMtutorial.ipynb) for the tutorial code.

To create the models we are using the `scikit-learn` library, written as `sklearn`.
 
### Steps to build and use a model:
* __Define:__ What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.
* __Fit:__ Capture patterns from provided data. This is the heart of modeling.
* __Predict:__ Just what it sounds like
* __Evaluate:__ Determine how accurate the model's predictions are.
  

NB: For the `random_state` you can use any number and the model quality won't be critically effected. 
  
  
## Model Validation
  
The most relevant measure of a model's quality is "predictive accuracy". I.e. how close the model's predictions are to what actually happens.
  
__Mean Absolute Error (MAE)__
`error = actual - predicted `
  
When using MAE, we take the absolute value of each error (A positive number), then we take an average of those absolute errors. The model can be said as:
* "On average, our predictions are off by about X."
  
`from sklean.metrics import mean_absolute_error`
`mean_absolute_error(y, predicted_home_prices)`
  
There are several issues with testing models on the training data ("In-Sample" scores). We must measure preformance on data that wasn't used to build the model. The easiest way to do this is to exclude some data from the model-building process, and then use those to test the model's accuracy on data it hasn't seen before. (This is __Validation data__)
  
Using the `scikit-learn` library:
  
`from sklearn.model_selection import train_test_split`
`train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)`
  
Then define and fit the model as usual using the `train_` variables, but test the model using the `val_` versions.
  
Once the "Out-of-sample" MAE is produced you can see how accurate the model is. In many cases the model must then be improved by experimenting to find better features or using different model types.
  
  
## Underfitting and Overfitting
  
* [Scikit-learn's documentation on decision tree models](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)
  
The most important options surrounding the decision tree model determine the tree's depth.  
  
__Overfitting__ is where a model matches the training data almost perfectly, but does poorly in validation and other new data. This occurs when we divide our data amongst many leaves, leaving fewer individual data points in each leaf. Leaves with very few data points may make very unreliable predictions for new data (because each prediciton is based on only a few pieces of data).
  
__Underfitting__ is where a model fails to capture important patterns and distinctions in the data, as such it performs badly even in training data. This occurs when each group/leaf has a wide variety of pieces of data. The resulting predictions may be far off for most pieces of data, even in the training data (it will also be bad in the validation data too for the same reason).
  
The aim is to find the "sweet spot" between underfitting and overfitting. This is the low point of the (red) validation curve below: 

![Mean Average Error against Tree Depth Graph](/files/MAEGraph.png)
  
_The photo above is taken from the [Kaggle Intro to Machine Learning Course](https://www.kaggle.com/learn/intro-to-machine-learning) all rights reserved to Kaggle and Dan Becker (Data Scientist - Instructor for this course)._
  
The `max_leaf_nodes` argument provides a very sensible way to control overfitting vs underfitting. Use a utility function to help compare MAE scores from differnt values for max_leaf_nodes. Using a for-loop we can then compare the accuracy of models built with different values for `max_leaf_nodes`.
  
In conclusion models can suffer from either:
* __Overfitting__: capturing spurious patterns that won't recur in the future, leading to less accurate predictions, or
* __Underfitting__: failing to capture relevant patterns, again leading to less accurate predictions.
  
__Validation__ data is used to measure a candidate model's accuracy. This allows us to test lots of candidate models and keep the best one.
  
  
## Random Forests
  
Decision trees bring up inevitable issues with underfitting and overfitting, lots of modern, sophisticated modeling techniques face the same tensions between underfitting and overfitting.
  
The __random forest__ uses lots of trees and it makes a prediction by averaging the predictions of each component tree. In general it has much better predictive accuracy than a single decision tree. It works well with the default parameters. Other models have even better performance, but are triciker to configure as they are more sensitive to getting the right parameters.
  
Created using the `RandomForestRegressor` class instead of `DecisionTreeRegressor`. See code in the notebook for this tutorial.
  
There is definitely still a lot of room for further improvement, but using the `RandomForestRegressor` is already a big improvement over the best decision tree error by over 50,000! There are parameters which allow you to improve the performance of the Random Forest in the same way that you can change the maximum depth of the single decision tree. One of the best features of Random Forest models are that they usually work fairly well even without this fine tuning.
  
  
## Machine Learning Competitions
  
The final section of the __Kaggle Intro to Machine Learning__ course was creating and submitting predictions for the [House Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course). I improved my initial submission score by finding the best value for `max_leaf_nodes` for this particular Random Forest model. (`max_leaf_nodes = 219`)
  
![Certificate for completing the "Intro to Machine Learning" course.](/files/EdwardSleath_Intro_to_ML.png)
  
  

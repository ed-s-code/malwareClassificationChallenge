# Kaggle Intermediate Machine Learning Notes
  
__NB: Parts of these notes are DIRECTLY COPIED from the [Kaggle Intermediate Machine Learning Course](https://www.kaggle.com/learn/intermediate-machine-learning), for example definitions etc.__

## Introduction
  
This course covers the following:
  
* Tackling data types often found in real-world datasets (__missing values, categorical variables, etc.__).
* Desigining __pipelines__ to improve the quality of machine learning code.
* Using advanced techniques for model validation (__cross-validation__).
* Building state-of-the-art models that are widely used to win Kaggle competitions (__XGBoost__).
* Avoiding common and important data science mistakes (__leakage__).
  
`n_estimators` is the number of trees you want to build before taking the averages of predictions. Higher number of trees gives better performance but makes the code slower.
  
Refined the Random Forest model using arguments such as `max_leaf_nodes` (Maximum depth of the trees), `n_estimators` (Number of trees ini the forest), `criterion` (Measures the quality of each split). Improved my submission for the [House Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course), jumping ~10k places on the leaderboard.
  
  
## Missing Values
  
Most ML libraries (including `scikit-learn`) give an error if you try to build a model using data with missing values. Below are three approaches to dealing with missing values:
  
1. __"The Simple Option": Drop Columns with Missing Values__
  
The easiest way to deal with missing values is to drop any columns that contain missing values. However, unless most values in the dropped columns are missing the model will lose access to a lot of potentially useful information.

2. __"The Better Option": Imputation__
  
__Imputation__ fills in the missing values with some number. For example, we can fill in the mean value along each column. This won't usually be right in most cases, but it leads to more accurate models than you would get from dropping the column entirely.
  
3. __An Extension to Imputation__
  
Imputation is the standard approach, but imputed values may be systematically above or below their actual values (which weren't collected in the dataset). The model would make better predictions by considering which values were originally missing. In this approach, we use imputation and, additionally, for each column with missing entries, we add a new column that shows the location of the imputed entries. Will either meaningfully improve results, or be pointless.
  
Use `SimpleImputer` to replace missing values with the mean value along each column. Whether this is effective or not varies by dataset. Using more complex ways to determine imputed values typically give no additional benefits once you plug the results in sophisticated machine learning models.
  
In the training data there are 12 columns with 10864 rows. 3 columns contain missing data but for each column, less than half of the entries are missing. Therefore, dropping the columns removes lots of useful information.
  
Improved my submission for the [House Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course) by using Approach 2 (Imputation), jumping ~6k places on the leaderboard.
  
  
## Categorical Variables
  
A __categorical variable__ takes only a limited number of values. I.e. where responses fall into a fixed set of categories (For example, where there are only 4 response options). Without preprocessing these types of variables most machine learning models will produce an error. There are 3 approaches that you can use to prepare your categorical data:
  
1. __Drop Categorical Variables:__ The easiest approach is again to simply remove these variables from the datasets. This approach will only work well if the columns did not contain useful information.
  
2. __Label Encoding:__ assings each unique value to a different integer. E.g. if the options are as follows: "Every day, Rarely, Most days, Never" then these could be assigned the values of "3, 1, 2, 0". The approach assums an ordering of the categories: "Never" (0) < "Rarely" (1) < "Most days" (2) < "Every day" (3). Not all categorical variables have a clear ordering in the values, but those that do are known as __ordinal variables__. For tree-based models, label encoding can be expected to work well with ordinal variables.
  
3. __One-Hot Encoding:__ creates new columns indicating the presence (or absence) of each possible value in the original data. For example where "colour" is a categorical variable with three categories: "Red", "Yellow", and "Green". The corresponding one-hot encoding contains one column for each possible value, and one row for each row in the original dataset. One-hot encoding does not assume an ordering of the categories. As such, you can expect this approach to work well if there is no clear ordering in the categorical data. These types of variables (Without an intrinsic ranking) are known as __nominal variables__. One-hot encoding does not work well if the categorical variable takes on a large number of values (generally won't use it for variables taking more than 15 different values). 
  
Select categorical columns by finding columns with relatibely low cardinality, i.e. where the number of unique values in a column is low (e.g. < 10).
  
Using the `select_dtypes()` method the categorical variables can be dropped.
  
Scikit-learn has a `LabelEncoder` class that can be used to get lable encodings. This is done by looping over the categorical variables and applying the label encoder separately to each column. For each column each unique value is randomly assined a different integer. This is a common approach but performance would increase if we provide better-informed labels for all ordinal variables.
  
Scikit-learn also has a `OneHotEncoder` class to get one-hot encodings. There are several key parameters to customise its behaviour:
  
* The `handle_unknown='ignore'` is used to avoid errors when the validation data contains classes that aren't represented in the training data.
* The `sparse=False` setting ensures that the encoded columns are returned as a `numpy` array (as opposed to a sparse matrix).
  
To use the encoder, we only supply the categorical columns that need to be one-hot encoded. In general, one-hot encoding will typically perform best, and dropping the categorical columns typically performs worst, but it varies on a case-by-case basis.

_"To calculate how many entries are added to the dataset through the one-hot encoding, begin by calculating how many entries are needed to encode the categorical variable (by multiplying the number of rows by the number of columns in the one-hot encoding). Then, to obtain how many entries are added to the dataset, subtract the number of entries in the original column."_

* The code below is used to fill NA/NaN values using the specified method in this case "ffil" (This propagates non-null values forward or backward)
```python 
X_test = X_test.fillna(method='ffill')
print("X_test " + str(X_test.shape))
``` 
  
Improved my submission for the [House Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course) by using Approach 3 (One-Hot Encoding), jumping ~10k places on the leaderboard.
  
  
## Pipelines
  
__Pipelines__ are a way to keep data preprocessing and code modeling organised. A pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step. Pipelines have some important benefits, including:
  
  1. __Cleaner code:__ When using a piepeline, you don't need to manually keep track of your training and validation data at each step.
  2. __Fewer Bugs:__ There are fewer opportuniteis to misapply a step or gorget a preprocessing step.
  3. __Easier to Productionise:__ It can be surprisingly hard to transition a model from a prototype to something deployable at scale. Pipelines help resolve some of these issues.
  4. __More Options for Model Validation:__ For example cross-validaiton which is covered in the next part of the course.
    
When using a pipeline you can easily deal with both categorical data and columns with missing values.
  
Use `ColumnTransformer` class to bundle together different preprocessing steps. Code to impute missing values in numerical data and code to impite missing values and apply a one-hot encoding to categorical data.
  
After defining the model, we can use the `Pipeline` class to define a pipeline that bundles the preprocessing steps. Key points to note:
  
* Using the pipeline, we preprocess the training data and fit the model in a single line of code. (This is as opposed to lots of separate steps which can become messy).
* When using the pipeline, we supply the unprocessed features in `X_valid` to the `predict()` command and the pipeline automatically preprocesses the features before generating any predictions. (Without a pipleine you have to remember to preprocess the validation data before making predictions.)
  
Pipelines are really useful for making sure that ML code is tidy and helps to avoid errors. Especially useful for workflows with sophisticated data preprocessing. I updated my submission for the [House Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course) by using pipelines and refined preprocessing and modeling definitions, again jumping ~10k places on the leaderboard.
  

## Cross-Validation
  
__Cross-Validation__ is used to better measure the performance of an ML model. There are several drawbacks to designing models in a data-driven way by measuring model quality with a validation set. In general, the larger the size of the validation set, the less randomness there is in the measure of model quality and as such the more reliable it will be. You can only get a large validation set by removing rows from our training data, and as such smaller training datasets mean worse models.
  
In __cross-validation__, the modeling process is run on different subsets of the data to get multiple measures of model quality. For example, the data could be divided into 5 pieces, each 20% of the full dataset. (In this case it is said that the data has been broken into 5 __"folds"__). The we run an experiemtn for each fold:
  
* In the first experiment, we use the first fold as the validation set and everything else as training data. The measure of model quality is based on a 20% holdout/validation set.
* In the second experiment, we use the second fold as the validation/holdout set and everyhting else as the training data. This produces a second estimate of model quality.
* This process is repeated, until we have used every fold once as the validaiton set. Combining these together, 100% of the data is used as the validation at some point, therefore we end up with a measure of model quality that is based on all the rows in the dataset (even though we don't use all the rows at the same time).
  
Cross-validation gives a much more accurate measure of model quality, this is especially important when making a lot of modeling decisions. However, it can take longer to run, because of it estimates multiple models (one per fold).
  
* _For small datasets_, where extra computational burden isn't that important, you should run cross-validation.
* _For larger datasets_, a single validation set is enough. The code will run faster, and the validation set will probably have enough data anyway.
  
As a general guide if the model takes a couple of minutes or less to run, it's probably a good idea to switch to cross-validaiton. Alternatively, if the scores for each experiment in cross-validaiton seem close/ yield similar results, a single validaiotn set is probably good enough.
  
Using a pipeline makes coding cross-validation much easier!
  
When using `neg_mean_absolute_error` with cross-validation remember to multiply by -1 because sklean calculates the negative MAE.
  
The `scoring` parameter chooses the measure of model quality to report. For cross-validation it is a good idea to choose negative mean absolut error (MAE) because scikit-learn has a convention where all metrics are defined so a high number is better. Using negatives allows them to be consistent with that convention. Usually want a single measure of model quality to compare alternative models, therefore we take the average across experiments.
  
Cross-validation yields a much better measure of model quality, and also helps to clean up the code! NB: No longer need to keep track of separate training and validation sets. Therefore, especially for small datasets, it's a good improvement!
  
__Grid search__ is a straightforward method of determinign the best combination of parameters for a machine learning model. `scikit-learn` contains a built-in function `GridSearchCV()` that makes grid serach code very efficient.
  

## XGBoost
  
__Gradient boosting__ allows you to build and optimise models to achieve "state-of-the-art" results on a variety of datasets.
  
So far in the course, we have been using the "Random Forest method" to make predictions, this is known as an "ensemble method". By definition, __ensemble methods__ combine the predictions of several models (e.g. several trees in the case of the random forest method). Another ensemble method is called gradient boosting.
  
__Gradient boosting__ is a method that goes through cycles to iteratively add models into an ensemble. Begins by initialising the ensemble with a single model, whose predictions can be pretty "naive". Subsequent additions to the ensemble address errors in prior models. The cycle is as follows:
  
* First, the current ensemble is used to generate predictions for each observation in the dataset. To make a prediction, we add the predictions from all models in the ensemble.
* These predictions are used to calculate a loss function (for example, a mean squared error).
* The loss function is used to fit a new model that is then added to the ensemble. The new model parameters are determined so that adding this new model to the ensemble will reduce the loss.
* The _gradient descent_ on the loss function is used to determine the parameters in this new model.
* The new model is added to the ensemble and the cycle repeats!
  
This example uses the `XGBoost` library which stands for __Extreme Gradient Boosting__. This is an implementation of gradient boosting which includes several additional features focused on performance and speed. The `XGBRegressor` class has many tunable parameters. NB: XGBoost doesn't work too well without tuning the parameters, these can dramatically affect accuracy and training speed. Several key parameters below:

* `n_estimators` specifies the number of times to go through the modeling cycle (described above). It is equal to the number of models that we include in the ensemble.
    * _Too low_ a value causes __underfitting__, inaccurate predictions on both training and test data.
    * _Too high_ a value causes __overfitting__, accurate predictions on training data, but inaccurate predictions on test data.
Typical values range from 100-1000, this depends on the `learning_rate` parameter below.
  
* `early_stopping_rounds` provides a way to automatically find the ideal value for n_estimators. This causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It is a good idea to set a high value for n_estimators and then use early_Stopping_rounds to find the optimal time to stop iterating. Setting `early_stopping_rounds=5` specifies that the cycle should stop after 5 straight rounds of deteriorating validation scores.
  
When using early_stopping_rounds, you need to also set aside some data for calculating the validation scores, this is done by setting the `eval_set` parameter. See the tutorial for code example. If you need to fit a model with all of your data, set n_estimators to whatever value was optimal when the early stopping was run.
  
* `learning_rate`: Instead of adding up the predictions from each component model, we can multiply the predictions from each model by a small number (known as the __learning rate__) before adding them in. This results in each tree that we add to the ensemvle being less useful. So, we can set a higher value for n_estimators without overfitting. If early stopping is being used, the appropriate number of trees will be determined automatically. 
  
In general using a small learning rate and a large number of estimators will yiled more accurate XGBoost models. However, it will also take the model longer to train since it does more iterators through the cycle. The default XGBoost `learning_rate = 0.1`.
  
* `n_jobs`: On larger datasets you can use parallelism to build models faster. It's common to set the parameter `n_jobs` to the number of cores on your machine. NB: For small datasets, this won't be useful. It's good in large datasets where otherwise there would be a long wait time during the `fit` command. 
  

__XGBoost__ is the primary library for working with standard tabular data (the type of data stored in Pandas DataFrames, as opposed to more exotic types of data like images and videos). With careful and precise parameter tuning, it can be used to train highly accurate models. It has some particularly useful functionality for machine learning with multiple GPUs.
  
I updated my submission for the [House Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course) to use the XGBoost model, jumping ~6k places on the leaderboard. __At the time of writing this I am ranked 4185/64360__.
  

## Data Leakage
  
__Data Leakage__ happens when the training data contains information about the target, but similar data will not be available when the model is used for prediction. This causes high performance on the training set (and potentially also the validation data), however the model will perform poorly in production. Leakage causes a model to look accurate until you start making decisions with the model, and which point the model becomes very innacurate.
  
Two main types of leakage:
  
#### Target Leakage:
  
__Target leakage__ occurs when the predictors include data that will not be available at the time you make predictions. It is very important to think about target leakage in terms of the timing/chronological order that data becomes available. (Not just whether a feature helps make good predictions).
  
For example, consider someone who will get sick with pneumonia. People take antibiotic medicine __after__ getting pneumonia in order to recover. However, the raw data may show a strong relationship between those columns, the `took_antibiotic_medicine` is frequently change _after_ the value for `got_pneumonia` is determined. This is target leakage. The model would see that anyone who has a value of `False` for `took_antibiotic_medicine` did not have pneumonia. Since the validation data comes from the same source as the training data, the pattern will repeat itself in validation, as such the model will have great validation (or even cross-validation) scores. However, the model will be very inaccurate when deployed in the real world.
  
To prevent target leakage, any variable updated (or created) after the target value is realised should be excluded.
  
#### Train-Test Contamination:
  
If validatoin is corrupted by validation data affecting the preprocessing behaviour, this is sometimes called __train-test contamination__. For example, if preprocessing occurs before calling `train_test_split()` the model may get good validation scores, btu perfomr poorly when deployed to make decisions. Data was incorporated from the validation/test data into how the predictions are made, it may do well on that particular data but not new data. This problem is even more critical when doing more complex feature engineering.
  
If the validation is based on a simple train-test split, the validation data should be excluded from any type of fitting, including the fitting of preprocessing steps. Using `scikit-learn` pipelines makes this a lot easier. It is especially critical to do the preprocessing inside the pipeline when using cross-validation.
  

It is very rare to find models that are accurate 98% of the time. It does occasionally happen but it is uncommon enough that the data should be inspected more closely for target leakage. Exclude any data that looks like it is a potential cause of data leakage as it is better to be safe than sorry. After removing data the accuracy may be quite a bit lower (e.g. down to ~ 80%). However, we can expect the accuracy to be right even when used on new applications, whereas the leaky model is likely to do much worse.
  
Careful separation of training and validation data cna prevent train-test contamination, using pipelines can help implement this separation. In turn a bit of caution, common sense, and data exploration can help identify target leakage.
  

![Certificate for completing the "Intermediate Machine Learning" course.](/files/EdwardSleath_Intermediate_ML.png)


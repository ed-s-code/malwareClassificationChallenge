# Kaggle Feature Engineering Notes
  
__NB: Parts of these notes are DIRECTLY COPIED from the [Kaggle Feature Engineering Course](https://www.kaggle.com/learn/feature-engineering), for example definitions etc.__

## What Is Feature Engineering?
  
One of the most important steps in building a good machine learning model is _feature engineering_. This course covers how to:
  
* Determine which features are the most important with __mutual information__.
* Invent new features in some real-world problem "domains".
* Encode high-cardinality (Lots of unique values) categoricals with a _target encoding_.
* Create segmentation features using _"k-means clustering"_
* Decompose a dataset's variatoin into features using _principle component analysis_.
  
  
The goal of feature engineering is to make any data better suited to the problem at hand. For example, "apparent temperature" measures like the heat index and the wind chill attempt to measure the perceived temperature to humans based on a variety of data source that can be directly measured like temperature, humidity and wind speed. An apparent temperature can be thought of as the result of a type of feature engineering (trying to make the observed data more relevant to what we actually are interested in - in this case how it actually feels outside). Feature engineering can be performed to:
* Improve a model's predictive performance.
* Reduce computational or data needs.
* Improve interpretability of results.
  
#### A Guiding Principle of Feature Engineering
  
A feature must have a relationship to the target that yoour model is able to learn in order to be useful. Linear models, are only able to learn linear relationships. Therefore, when using a linear model the goal would be to transform the features to make thei relationship to the target linear. Any transformation you apply to a feature becomes a part of the model itself.
  
For example, if we are trying to predict the `Price` of square plots of land from the `Length` of one side. This relationship is not linear, so fitting a linear model directly to `Length` would give poor results. However, if we square the `Length` feature to get `Area` this creates a linear relationship. Adding the new `Area` feature means the linear model can now fit a parabola. Essentially, squaring a feature gave the linear model the ability to fit squared features.
  
The above example demonstrates why feature engineering can be so rewarding. If our model can't learn any particular relationships, we can provide these through transformations. It is good to think about what information models can use to achieve the best performance.
  
The code for this tutorial shows how adding a few "synthetic" features can improve the performance of a random forest model. Adding some additional features derived from pre-existing features can help a model to learn important relationships.
  
It is useful to establish a vaseline by training the model on an un-augmented dataset. This helps to determine whether any new features are actuallly useful. Establishing a baseline is good practice at the start of the feature engineering process.
  
In cooking the ratios of ingredients in a recipe is usually a better predictor of how the recipe will turn out than their absolute amounts. Looking at the ratios between features is often a good starting point in feature engineering.
  
Learning how to identify features in the dataset that might be useful to combine is important, this is done with _mutual information_.
  

## Mutual Information.
  
A good first step when encountering a new dataset is constructing a ranking with a __feature utility metric__. This is a function that measures associations between a feature and the target. From there you can select a smaller set of the most useful features to trial and develop initially. One of the metrics used is called "mutual information". __Mutual information__ is similar to correlation as it measures the relationship between 2 quantities. Mutual information can detect _any_ kind of relationship, whereas correlation only detects _linear_ relationships. Mutual information is:
  
* Easy to use and interpret.
* Computationally efficient
* Theoretically well-founded
* Resistant to overfitting
* Able to detect any kind of relationship.
  
__Mutual information (MI)__ describes relationships in terms of uncertainty. The mutual information between 2 quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. (I.e. if you knew the value of a feature, how does you confidence in the target value increase.)
  
For example, when looking at housing: knowing the value of something like `ExterQual` (Exterior Quality) may allow you to be more certain about the corresponding `SalePrice`. E.g. each category of ExterQual may concentrate SalePrice to within a certain range. The mutual information that `ExterQual` has with `SalePrice` is the average reduction of uncertainty in `SalePrice` taken over the values of `ExterQual`. If one `ExterQual` value occurs less often than another, this is recognised in the weighting of the MI score.
  
__Technical Note (Copied directly from the course, relating to "uncertainty"):__ What we're calling uncertainty is measured using a quantity from information theory known as "entropy". The entropy of a variable means roughly: "how many yes-or-no questions you would need to describe an occurance of that variable, on average." The more questions you have to ask, the more uncertain you must be about the variable. Mutual information is how many questions you expect the feature to answer about the target.
  
#### Interpreting Mutual Information Scores
  
The lowest possible mutual information between quantities is 0.0, this is when the quantities are independent (neither can tell us anything about the other). In theory, there is no upper bound to the value of MI however, values above ~2.0 are uncommon. (MI is a logarithmic quantity).
  
The graph below shows how MI values correspond to the kind and degree of association a feature has with the target. NB: Mutual information increases as the dependence between a feature and target becomes tighter. MI can capture any kind of association.
  
![Mutual Information Comparison Graph](/files/MIGraph.png)
  
_The photo above is taken from the [Kaggle Feature Engineering Course](https://www.kaggle.com/learn/feature-engineering) all rights reserved to Kaggle and Ryan Holbrook (Data Scientist - Instructor for this course)._
  
Key things to remember when applying mutual information:
  
* MI is helpful when trying to understand the _relative potential_ of a feature as a predictor of the target.
* MI is a __univariate__ metric as it can't detect interactions between features. (A feature may be very informative when interacting with other features, but not on its own.)
* The actual usefulness of a feature is depends on the model it is used with. A feature is only useful as long as its relationship to the target is one the model can learn. Therefore, just because a featuer has a high MI score does not mean that the model will be able to do anything with that. Transformation may be required to strengthen any associations.
  
In the code tutorial, an automile dataset is being analysed. The features are ranked using mutual information and the results are investigated using data visualisation. 
  
`scikit-learn` treats discrete features differently from continuous features. Need to tell the algorithm which are which. Any that _must_ have a `float` dtype is _not_ discrete. Categoricals (`object` or `categorial` dtype) can be treated as discrete by using label encoding.
  
Scikit-learn has 2 mutual information metrics in the `feature_selection` module:
  
* `mutual_info_regression` is for real-valued targets.
* `mutual_info_classif` is for categorical targets.
  
Data visualisation is a really good follow-up to a utility ranking. By using both utility metrics (e.g. mutual information) and visualisations it is easier to discover important relationships in the data.
  
Looking at high scoring MI features allows you to draw out key themes and trends across the data. Combining the top features with other, related features (especially those that create interactions) is a good strategy for coming up with a really useful set of features to train models on.
  


## Creating Features
  
Once a potential set of features have been identified, they can then be developed. This section of the course goes over a number of common transformations that can be done using `Pandas`.
  
##### Tips on Discovering New Features:
  
* Understand the data. Look over the dataset's _data documentation_, if available.
* Research the problem domain to acquire __domain knowledge__. Wikipedia is a good starting point, alongside books and journal articles.
* Study previous work. Looking over write-ups of other people's solutions to other datasets or the current dataset can be very useful.
* Using data visualisaiton can reveal relationships and distributions in the data that could be simplified. Data visualisation is very important!
  
#### Mathematical Transformations
  
Relationships between numerical features are frequently expressed using mathematical formulae. These formulae will usually come up as part of the domain research. Using `Pandas` you can apply arithmetic operations to entire columns. 
  
The automobile dataset includes features describing a car's engine. There are a variety of formulas relating to this which can potentially be used to create useful new features. (For example, the "stroke" ratio). The more complicated a combination/formula is, the more difficult it will be for the model to learn. Data visualisation can suggest transformations. (For example "reshaping" a feature through powers or logarithms)
  
#### Counts
  
Features which describe the presence/absence of something usually come in sets (e.g. the set of risk factors for a disease). You can aggregate these sort of features by creating a __count__. These features are usually in binary (1 = Present, 0 = Absent) or boolean (True = Present, False = Absent). NB: In python, booleans can be added up like integers.
  
The traffic accidents dataset includes several features which indicate whether a roadway object was near the accident. These can be used to create a count of the total number of roadway features nearby.
  
A dataframe's built-in methods can be used to create boolean values. (In the example of the concrete dataset, this can keep track of the formulations which lack one or more components. This can count how many components are in a formulation with the dataframe's built in `gt` greater-than method).
  
#### Building-Up and Breaking-Down Features
  
There will frequently be features that contain complex strings which can usefully be broken into simpler pieces. For example: (Used example data from the course)
  
* ID Numbers: `'123-45-6789'`
* Phone Numbers: `'(999) 555-0123'`
* Street addresses: `'8241 Kaggle Ln., Goose City, NV'`
* Internet addresses: `'https://www.kaggle.com/ryanholbrook/creating-features'`
* Product codes: `'0 36000 29145 2'`
* Dates and times: `'Mon Sep 30 07:06:05 2013'`
  
Features like these often have a kind of structure that you can use. Landline phone numbers for example often have an area code which tells you the rough location of the caller. The `str` accessor allows you to apply string methods like `split` to columns. For example, in the customer dataset, the `policy` feature can be separated into `type` and `level`.
  
Similarly, features could be joined if you think there is some level of interaction in the combination. For example combining car `make` and `body_style` features into a `make_and_style` feature.
  
Other types of information rich data include: Dates and times, latitudes and longitudes, and text!
  
#### Group Transformations
  
__Group transformations__, aggregate information across multiple rows grouped by some category. Using a group transformation you can create features like "the average income of a person's state of residence", or "the proportion of moves released on a weekday, by genre". If a category interaction has been discovered, a group transformation over that category might be a good idea.
  
A group transformation uses an aggregation function to combine 2 features:
  
* A categorical feature (that provides the grouping)
* Another feature (whose values you wish to aggregate)
  
For example, for "average income by state": `State` is the grouping feature, `mean` would be the aggregation function, and `Income` is the aggregated feature. This can all be computed using `Pandas` with the `groupby` and `transform` methods. The `mean` function is a built-in dataframe method, as such it can be passed as a string to transform. Other such methods include:
  
`max`, `min`, `median`, `var`, `std`, and `count`.
  
Group transformations using methods like `count` can be used to create a "frequency encoding" for a categorical feature. 
  
NB: if using training and validation data splits, so that their independence is preserved, it is best to create a grouped feature using only the training set and then join it to the validation set. Use the `drop_duplicates` method on the training set to create a unique set of values and then the validation set's `merge` method to combine them.
  

##### Tips on Creating Features:
  
* Linear models can't learn anything more complex than sums and differences (which they learn naturally).
* Ratios are difficult for most models to learn. Ratio combinations can lead to easier performance gains.
* Linear models and neural nets usually do better with normalised features. Neural nets needs features scaled to values close to 0. Tree-based models (e.g. random forests and XGBoost) can sometimes benefit from normalisation, but not by very much.
* Tree models can learn to approximate almost all combinations of features however, when a combination is particularly important they can benefit from having it created for them, especially when data is limited.
* Counts are particularly useful for tree models, as these models don't have any natural ways of aggregating information across lots of features at once.
  
  
  
## Clustering With K-Means
  
Unsupervised learning algorithms don't use a target; instead, their purpose is to learn some property of the data and to represent the structure of the features in a certain way. In terms of feature engineering for prediciton, an unsupervised algorithm is a "feature discovery" technique.
  
__Clustering__ is the assigning of data points to groups based upon how similar the points are to each other. When used for feature engineering, we can try to discover groups of customers representing a market segment for example, or geographical areas that share similar weather patterns. Adding a feature of cluster labels helps machine learning models to untangle complicated relationships of space/proximity.
  
#### Cluster Labels as a Feature:
  
When applied to a single real-valued feature, clustering acts like a traditional "binning" or "discretisation" transformation. When applied to multiple features, it is like "multi-dimensional binning" (sometimes known as _vector quantisation_). The graph below shows how clustering works. On the left: Clustering a single feature. And on the right: Clustering across two features.
  
![Clustering Example Image](/files/ClusteringExample.png)
  
_The photo above is taken from the [Kaggle Feature Engineering Course](https://www.kaggle.com/learn/feature-engineering) all rights reserved to Kaggle and Ryan Holbrook (Data Scientist - Instructor for this course)._
  
NB: the `Cluster` feature is categorical. it can be shown with a lebel encoding (sequence of integers) as a typical clustering algorithm would produce. However, depending on the model being used, a one-hot encoding may be more appropriate.
  
Cluster labels are added in order to break up complicated relationships across features into simpler chunks. The model can then learn the simpler chunks one-by-one, instead of attempting to learn the complicated overall relationship. It is a "divide and conquer" strategy. Clustering can be used to improve a simple linear model: a curved relationship between two features is too complicated for this kind of model (it underfits). When looking at smaller chunks however, the relationship is almost linear and as such can be easily learnt by the model.
  
#### k-Means Clustering
  
There are lots of clustering algorithms, primarily differing in how they measure "similarity" or "proximity" and in what kinds of features they work with. K-means is intuitive and easy to apply in a feature engineering context. NB: depending on the application another algorithm may be more appropriate.
  
__K-means clustering__ measures similarity by using ordinary straight-line distance (Euclidean distance). It creates clusters by placing several points (__centroids__) inside the feature-space. Each point in the dataset is then assigned to the cluster of whichever centroid it is closest to. The "k" in "k-means" refers to the number of centroids/clusters it creates. The k is self defined. You can picture each centroid capturing their points through a sequencing of radiating circles. When sets of circles from different, competing centroids overlap they form a line. The result is called a __Voronoi tessallation__. The tessallation shows you to what clusters future data will be assigned (it is basically what k-means learns from its training data).
  
There are 3 key parameters in scikit-learn's implementation of the k-means clustering algorithm: `n_clusters`, `max_iter`, and `n_init`. It is a 2-step process. The algorithm begins by randomly initialising some predefined number (`n_clusters`) of centroids. The algorithm then iterates over the following 2 operations:
  
1. Assing points to the nearest cluster centroid.
2. Move each centroid to minimise the distance to its points.
  
These 2 steps are repeated until the centroids no longer move, or until some maximum number of iterations has passed (`max_iter`). What usually happens is that the initial random postion of the centroids ends in a poor clustering. Because of that the algorithm repeats a number of times (`n_init`) and completes by returning the clustering that has the least total distance between each point and its centroid (the optimal clustering). The .gif below shows the algorithm in action:
  
![The k-means algorithm in action](/files/k-means.gif)
  
_The gif above is taken from the [Kaggle Feature Engineering Course](https://www.kaggle.com/learn/feature-engineering) all rights reserved to Kaggle and Ryan Holbrook (Data Scientist - Instructor for this course)._
  
It may be necessary to increase the `max_iter` for a large number of clusters or the `n_init` for a complex dataset. Usually however, the only self defined parameter will be `n_clusters` (the "k" value). The best partitioning for a set of features is dependent on the model being used and what is being predicted. It is a good idea to tune it like any hyperparameter (for example, through cross-validaiton).
  
NB: K-means clustering is sensitive to scale, as such it is often a good idea to rescale or normalise data with extreme values.
  
Use data visualisation to measure how effective the k-means clustering was. For example, a scatter plot showing the geographic distribution of clusters. If the clustering is informative, the distibutions should, in general, separate across the target for the dataset.
  
NB: If features don't have comparable units they should be rescaled. (E.g. `Number of Doors` and `Horsepower` of a 1989 model car.)
  
The k-means algorithm offers an alternative way of creating new features. INstead of labelling each feature with the nearest cluster centroied, it can measure the distance from a point ot all the centroids and return those distances as features.
  
  
  
## Principal Component Analysis
  
__Principal Component Analysis (PCA)__ is another model-based method for feature engineering. PCA is like a partitioning of the variation in data, it is a great tool that can be used to discover important relationships in the data and create more informative features.
  
NB: PCA is typically applied to standardised data! (With standardised data "variation" means "correlation". With unstandardised data "variation" means "covariance".)
  
Using an "Abalone" dataset as an example: within this dataset are physical measurements taken from several thousand Tasmanian abalone. Consider that within this data there are "axes of variation" that describe the different ways that abalone tend to differ from one another. Visually, these axes appear as perpendicular lines running along the naturla dimenions of the data, one axis for each original feature. These axes of variation can be given names:
  
For example, the longer axis can be called the "Size" component: small height and small diameter vs large height and large diameter. The shorter axis may be called the "Shape" componenet: small height and large diameter vs large height and small diameter.
  
In this case of abalones, we can easily describe them by their `'Size'` and `'Shape'` as opposed to their `'Height'` and `'Diameter'`. This is the main idea of PCA: instead of describing the data using the original features, we can describe it with its axes of variation. (The axes of variation become the new features). The new features PCA constructs are simply linear combinations (weighted sums) of the original features. For example:
  
`df["Size"] = 0.707 * X["Height"] + 0.707 * X["Diameter"]`
`df["Shape"] = 0.707 * X["Height"] - 0.707 * X["Diameter"]`
  
The new features are called the __principal components__ of the data. The weights are called __loadings__. There will be as many principal components as there are features in the original dataset. The component's weighting/loadings tell us what variation it expresses through signs and magnitudes.
  
PCA gives us the amount of variation in each component. There is sometimes more variation in the data along one component than along another component. PCA makes this precise by using each component's __percentage of explained variance__. NB: The amount of variance in a component doesn't always correspond to how good it is as a predictor. (Depend on what you're trying to predict).
  
#### PCA for Feature Engineering
  
There are 2 ways PCA can be used for feature engineering:
  
1. As a descriptive technique. As the componenets tell you about the variation, you can compute the MI (Mutual Information) scores for the componenets and analyse what kind of variation is most predictive of your target. This helps to decide what features to create (for example, either the product of `'Height'` and `'Diameter'` if `'Size'` is important or a ratio of `'Height'` and `'Diameter'` if '`Shape'` is important). It may even be beneficial to try clustering `+ of the high-scoring components!
  
2. Use the components themselves as features. This is because the components expose the variational structure of the data directly. They therefore can frequently be more informative than the original features. Several use-cases:
  
    * __Dimensionality reduction:__ Where the features are highly redundant (multicollinear, specifically), PCA can partition out the redundancy into `+ near-zero variance components. These can then be dropped as they contain little/no information.
    * __Anomaly detection:__ Unusual variation, which isn't apparent from the original features, can often show up in the low-cariance components. These components can be highly informative in an anomaly or outlier detection task.
    * __Noise reduction:__ A group of sensor readings will often share common background noise. PCA can sometimes be used to collect the (informative) signal into a smaller group of features while ignoring the noise. This boosts the signal-to-noise ratio.
    * __Decorrelation:__ Some ML algorithms have difficulty with highly-correlated features. PCA can transform correlated features into uncorrelated components, this can then be easier for an algorithm to work with.
  
PCA basically provides direct access to the correlational structure of your data.
  
##### PCA Best Practices:
  
Useful things to keep in mind when applying PCA:
* PCA only works with numerical features (e.g. continuous quantities or counts).
* PCA is sensitive to scale. (Good practice to standardise data before applying PCA).
* Consider removing/constraining outliers (They can have an undue influence on model results).
  
  
The code tutorial uses the "Autombile" dataset and applies PCA, using the descriptive technique to discover features. This uses `scikit-learn`'s `PCA` estimator.
  
NB: terminology is inconsistent. Good practice to follow a convention such that transformed columns in `X_pca` are called the components. As such after fitting the model, the `PCA` isntance contains the loadings in its `components_` attribute. The loadings are wrapped in a dataframe.
  
  
  
## Target Encoding
  
Target encoding is designed fro categorical features. It is a method of encoding categories as numbers, similar to one-hot or label encoding. The difference is that it also uses the target to create the encoding. Target encoding is known as a __supervised__ feature engineering technique.
  
__Target encoding__ is any form of encoding that replaces a feature's categories with some number derived from the target. One simple and effective version of this is to apply a group aggregation, like the "mean". This method of target encoding is sometimes known as __mean encoding__. When applied to a binary target, it is also called __bin counting__. (Other names include: likelihood encoding, impact encoding, and leave-one-out encoding.)
  
#### Smoothing
  
Encoding like this brings up a few issues:
  
1. _Unknown categories_: Target encodings cause a special risk of overfitting, as such they must be trained on an independent "encoding" split. When joining the encoding to future splits, `Pandas` will fill in any missing values for categories not present in the encoding split. (These values would need to be imputed somehow).
  
2. _Rare categories_: When a category only occurs a couple of times in the dataset, any stats calculated on its group are likely to be inaccurate. For example, in the "Automobiles" dataset, the `'mercury'` make only occurs once. The "mean" price caluclated of this group is simply the price of that one vehicle. This is obviously not likely to be very representative of any Mercuries that are seen in the future. Target encoding these "rare categories" increase the chance of overfitting.
  
A solution to the problems mentioned above is to add __smoothing__. The idea with smoothing is to blend the in-category average with the overall average. This means rare categories get less weight on their category average, while missing categories just get the overall average. In pseudocode form:
  
` encoding = weight * in_category + (1 - weight) * overall `
  
The `weight` is a value between 0 and 1. This is calculated from the category frequency. An easy method of determining the value for `weight` is to compute an __m-estimate__:
  
` weight = n / (n + m) `
  
`n` is the total number of times the category occurs in the data. The parameter `m` determines the "smoothing" factor. The larger the value of `m` the more weight is put on the overall estimate. When choosing a value for `m`, think about how noisy the categories might be. Is a lot of data needed in order to get good estimates? If so, it may be a better choice to choose a larger value for `m`.
  
##### Use Cases for Target Encoding:
  
* __High-cardinality features:__ These are features with a large number of categories. They are often tricky to encode (for example, one-hot encoding would generate too many features and alternatives, similar to a label encoding, it might not be appropriate for that feature). A target encoding derives the numbers fro the categories using the feature's _most important_ property! (Its relationship with the target!)
* __Domain-motivated features:__ From previous experience, you may suspect that a categorical feature should be important to the model (even if it scored poorly with a feature metric). Using a target encoding can help to reveal a feature's treu informativeness.
  
  
The code tutorial for this section of the course uses the "MovieLens1m" dataset which contains 1 million movie ratings by users of the "MovieLens" website.
  
You can use the `category_encoders` package in `scikit-learn-contrib` to implement an m-estimate encoder.
  
  
![Certificate for completing the "Feature Engineering" course.](/files/EdwardSleath_FeatureEngineering.png)
